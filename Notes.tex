\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}

\title{Cornell Machine Learning Notes}
\author{Bryan Gass}
\date{November 24 2020}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Supervised Learning} 
    high level overview of supervised machine learning: attempts to make predictions from data 
    
\subsection{Setup}
    We are given a dataset $\textbf{D}$ within data set we have data points and their labels, where $x_i$ is D-dimensional vector. $x_i$ is considered to be an input instance/attribute/feature/covariate. $x_i$ also can be something much more complex like an image, a sentence or email, or molecular shape.  $y_i$ is the label associated with that feature vector/input. There are also $\textbf{n}$ data points within a dataset, $\textbf{D}$. \\
    
    It should be noted the difference between x and $x_i$ as well as y and $y_i$. $x_i$ can be thought of the $\textbf{feature vector}$ that we are looking to train our model on, while generally speaking x is the feature vector that behaves as a testing input, thus why we refer to it as a $\textbf{test input}$. This idea carries over to y and $y_i$. $y_i$ is the label that we use to check the validity of our feature vector $x_i$. y is the output or the $\textbf{response variable}$ that is associated with the test input. \\
    
    \begin{equation}
        D = {(x_1, y_1), ..., (x_n, y_n)}
    \end{equation}
    
    \begin{itemize}
        \item 
            $\textbf{\textit{R}}^{d}$ is the d-dimensional feature space
            
        \item
            $x_i$ is the input vector of the $i^{th}$ sample
        
        \item   
            $y_i$ is the label of the $i^{th}$ sample
            
        \item
            \textit{C} is the label space \\
    \end{itemize} 
    
    The arbitrary data point $x_{i}$ and the label $y_{i}$ is assumed to be under the normal distribution P. It should be noted we do not have access to this normal distribution, P, ever. It is a hypothetical normal distribution that contains the theoretical total amount of data of the dataset. Where our dataset $\textbf{D}$ is within P.
    \begin{equation}
        (x_{i}, y_{i}) \sim P
    \end{equation} \\
    
    \textbf{The Idea of Supervised Learning:} to take our data set that contains the data points, $x_{i}$, and our labels ,$y_{i}$, to "learn" a function that can goes from $x_{i}$ to $y_{i}$. This also can be thought of as trying to "learn" a model such that it can adequately map  inputs x to outputs y, given by a input-output pair, our training set. Denoted as: \\
    
    \begin{equation}
        D = {(x_{i}, y_{i})}_{i=1}^N
    \end{equation}
    
    Where D is called the training set and N is the number of training examples 
    \\
    
    Term 1: \textbf{i.i.d.} - Independent And Identically Distributed Random variables 
    \\
    
    \subsection{Supervised Learning:}
    Most methods will assume that $y_i$, the label, is either a categorical or nominal variable. given $y_{i} \in {1,..., C}$ When $y_i$ is categorical (male or female) then it is considered a $\textbf{classification problem}$. When $y_i$ is a real valued scalar (like house prices) is considered to be a $\textbf{regression problem}$. Another variant of this is when $y_i$ has some type of natural ordering to it, such as grade values from A to F, this is considered to be $\textbf{ordinal regression}$.   
    
    \begin{itemize}
        \item 
            \textbf{Binary Classification:} 
            if $C=2$ then this is considered to be a binary classification problem
            \begin{equation}
                {y \in {0, 1}} \text{ or } {y \in {-1, +1}}
            \end{equation}
            Predicting whether or not a given input belongs to one of two classes, typically in terms of true or false, or positive or negative. (e.g., face detection). 
            
                \begin{enumerate}
                    \item Example 1: 
                        Given a classifier that determines if something is spam or not we could view x as the email and y as a label that is either "spam" or "not spam"
            
                    \item Example 2:
                        Given a classifier that determines if the stock market is rising or falling we can view the x as the stock and the y as the label that is either "up" or "down"
                \end{enumerate}
            
        \item
             \textbf{Regression:} 
            \begin{equation}
                {Y \in \rm I\!R}
            \end{equation}
            Predicting a real number. (e.g., predicting gas prices).
            
        \item
             \textbf{Multi-class Classification:} 
            \begin{equation}
                {y \in {1, ..., C}} \text{ Where C is a discrete number. }
            \end{equation}
            Predicting which of {K} classes an input belongs to. (e.g., face recognition). 
            
        \item 
            \textbf{Multi-label Classification}
            When there are many class labels that are not mutually exclusive to one another (tall but not strong). Generally speaking this is comparing many non-mutually exclusive binary classifications to one another, but is not always the case. 
    \end{itemize}
    
    \subsection{Unsupervised Learning:}
        Within unsupervised learning we are only given the feature vectors $x_i$, without the label $y_i$, where we are tasked to find "interesting patterns". This is often called $\textbf{Knowledge Discovery}$. This is denoted by:
        \begin{equation}
            D = {(x_i)}_{i=1}^N
        \end{equation}
    
    \subsection{Training vs. Testing Data}
        \begin{itemize}
            \item 
                \textbf{Training:}
                A learner {L} is given a set of training data {(x1,y1),…,(xn,yn)} and allowed to perform computations on the training data to learn an output function {h(x)}.
                
            \item 
                \textbf{Testing:}
                {L} is asked to generate predictions {$h(x_1)$, ..., $h(x_m)$} for a set of testing data. We can then compute an error metric or loss function to determine quantitatively if the learner correctly predicted the true outputs for the test examples. A common metric for classification tasks is the 0–1 loss, which is just the proportion of incorrect guesses
                
        \end{itemize}
    
    \subsection{Hypothesis Functions}
    A list of arbitrary functions that we the scientists can pick from that we believe will best be able to fit the data. That is, the function that is best at taking and input and finding the associated label within a test setting
    
    we can describe the function we choose to be h within the arbitrarily large list of H functions which are known to be \textbf{Hypothesis Classes}.
    
    \subsection{Loss Functions:}
    \begin{enumerate}
        \item \textbf{0/1 Loss Function}:
            We run this particular h over our dataset, D.
            \begin{equation}
                L_{0/1}(h) = \frac{1}{n}\sum_{i=1}^{n} \delta_{h(x_i)\neq y}
            \end{equation}
            where
            \begin{equation}
                \delta_{h(x_i)\neq y} = \begin{cases}
                                            1,& \text{if } h(x_i)\neq y\\
                                            0,& \text{otherwise}
                                        \end{cases}
            \end{equation}
            For every single example it suffers a loss of 1 if it is mispredicted, and 0 otherwise. The normalized zero-one loss returns the fraction of misclassified training samples, also often referred to as the training error.
            
            The loss function returns the error rate on this data set D. 
            
        \item \textbf{Squared Loss:}
            Typically used within regression, the square loss can be thought of as prediction minus the actual value. However there are trade offs made by using the square loss function. The loss suffered is always non-negative. Additionally, due to the square loss being quadratic that means outliers that made be off by a lot may make it seem like this function is doing worse than, lets say, a lot of points off by a little. 
            
            \begin{equation}
                h(x) = E_{P(y\mid x)}[y]
            \end{equation}
            \begin{equation}
                L_{sq}(h) = \frac{1}{n} \sum_{i=1}^{n} (h(x_i)-y_{i})^2
            \end{equation}
            
        \item \textbf{Absolute Loss:}
            to remedy the effect outliers within our potential dataset, D, we can you the absolute loss function that will not square the result but rather take the absolute value. This means that the suffered loss grows linearly 
            
            \begin{equation}
                h(x) = \textbf{MEDIAN}_{P(y\mid x)}[y]
            \end{equation}
            \begin{equation}
                L_{abs}(h) = \frac{1}{n} \sum_{i=1}^{n} \mid h(x_i)-y_{i} \mid
            \end{equation}
    \end{enumerate}
    
\section{k-Nearest Neighbors}
    Remarks to keep in mind:
    \begin{itemize}
        \item 
            choose an odd k value for a 2 class problem
            
        \item
            k must not be a multiple of the number of classes
            
        \item
            the main drawback of kNN is the complexity in searching the nearest neighbors for each sample
    \end{itemize} \\
    
    
        
    
    
\end{document}
